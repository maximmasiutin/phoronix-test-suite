<?xml version="1.0"?>
<!--Phoronix Test Suite v10.8.5-->
<PhoronixTestSuite>
  <TestInformation>
    <Title>Llama.cpp</Title>
    <AppVersion>b6401</AppVersion>
    <Description>Llama.cpp is a port of Facebook's LLaMA model in C/C++ developed by Georgi Gerganov. Llama.cpp allows the inference of LLaMA and other supported models in C/C++. For CPU inference Llama.cpp supports AVX2/AVX-512, ARM NEON, and other modern ISAs along with features like OpenBLAS usage.</Description>
    <ResultScale>Tokens Per Second</ResultScale>
    <Proportion>HIB</Proportion>
    <TimesToRun>3</TimesToRun>
  </TestInformation>
  <TestProfile>
    <Version>2.2.0</Version>
    <SupportedPlatforms>Linux, Windows</SupportedPlatforms>
    <SoftwareType>Utility</SoftwareType>
    <TestType>System</TestType>
    <License>Free</License>
    <Status>Verified</Status>
    <ExternalDependencies>build-utilities, blas-development, cmake</ExternalDependencies>
    <InstallRequiresInternet>TRUE</InstallRequiresInternet>
    <EnvironmentSize>58700</EnvironmentSize>
    <ProjectURL>https://github.com/ggerganov/llama.cpp/</ProjectURL>
    <RepositoryURL>https://github.com/ggerganov/llama.cpp</RepositoryURL>
    <Maintainer>Michael Larabel</Maintainer>
    <SystemDependencies>pkgconf</SystemDependencies>
  </TestProfile>
  <TestSettings>
    <Option>
      <DisplayName>Backend</DisplayName>
      <Identifier>backend</Identifier>
      <Menu>
        <Entry>
          <Name>CPU BLAS</Name>
          <Value>BLAS</Value>
        </Entry>
        <Entry>
          <Name>NVIDIA CUDA</Name>
          <Value>CUDA -r 100 -fa 1</Value>
        </Entry>
        <Entry>
          <Name>AMD ROCm HIP</Name>
          <Value>ROCM -r 100 -fa 1</Value>
        </Entry>
        <Entry>
          <Name>Vulkan</Name>
          <Value>VULKAN -r 100 -fa 1</Value>
        </Entry>
      </Menu>
    </Option>
    <Option>
      <DisplayName>Model</DisplayName>
      <Identifier>model</Identifier>
      <ArgumentPrefix>-m ../</ArgumentPrefix>
      <Menu>
        <Entry>
          <Name>Llama-3.1-Tulu-3-8B-Q8_0</Name>
          <Value>Llama-3.1-Tulu-3-8B-Q8_0.gguf</Value>
        </Entry>
        <Entry>
          <Name>granite-3.0-3b-a800m-instruct-Q8_0</Name>
          <Value>granite-3.0-3b-a800m-instruct-Q8_0.gguf</Value>
        </Entry>
        <Entry>
          <Name>Mistral-7B-Instruct-v0.3-Q8_0</Name>
          <Value>Mistral-7B-Instruct-v0.3-Q8_0.gguf</Value>
        </Entry>
        <Entry>
          <Name>gpt-oss-20b-Q8_0</Name>
          <Value>gpt-oss-20b-Q8_0.gguf</Value>
        </Entry>
        <Entry>
          <Name>DeepSeek-R1-Distill-Llama-8B-Q8_0</Name>
          <Value>DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf</Value>
        </Entry>
        <Entry>
          <Name>Qwen3-8B-Q8_0</Name>
          <Value>Qwen3-8B-Q8_0.gguf</Value>
        </Entry>
      </Menu>
    </Option>
    <Option>
      <DisplayName>Test</DisplayName>
      <Identifier>test</Identifier>
      <Menu>
        <Entry>
          <Name>Text Generation 128</Name>
          <Value>-n 128 -p 0</Value>
        </Entry>
        <Entry>
          <Name>Prompt Processing 512</Name>
          <Value>-n 0 -p 512</Value>
        </Entry>
        <Entry>
          <Name>Prompt Processing 1024</Name>
          <Value>-n 0 -p 1024</Value>
        </Entry>
        <Entry>
          <Name>Prompt Processing 2048</Name>
          <Value>-n 0 -p 2048</Value>
        </Entry>
      </Menu>
    </Option>
  </TestSettings>
</PhoronixTestSuite>
